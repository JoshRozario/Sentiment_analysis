{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "source": [
    "import os\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.svm import LinearSVC\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "import nltk\r\n",
    "import string\r\n",
    "from nltk.tokenize import ToktokTokenizer\r\n",
    "import re \r\n",
    "import spacy\r\n",
    "from autocorrect import Speller"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "print(os.listdir(\"./input\"))\r\n",
    "\r\n",
    "imdb = pd.read_csv('./input/imdb_labelled.txt', sep ='\\t', header = None)\r\n",
    "\r\n",
    "imdb.columns = [\"sentence\",  \"sentiment\"]\r\n",
    "\r\n",
    "amazon = pd.read_csv('./input/amazon_cells_labelled.txt', sep ='\\t', header = None)\r\n",
    "\r\n",
    "amazon.columns = [\"sentence\",  \"sentiment\"]\r\n",
    "\r\n",
    "yelp = pd.read_csv('./input/yelp_labelled.txt', sep ='\\t', header = None)\r\n",
    "\r\n",
    "yelp.columns = [\"sentence\",  \"sentiment\"]\r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['amazon_cells_labelled.txt', 'imdb_labelled.txt', 'yelp_labelled.txt']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "source": [
    "#Strip functions\r\n",
    "\r\n",
    "tokenizer = ToktokTokenizer()\r\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\r\n",
    "# custom: removing words from list\r\n",
    "stopword_list.remove('not')\r\n",
    "\r\n",
    "# call function\r\n",
    "\r\n",
    "def remove_punctuation(text):\r\n",
    "    text = ''.join([c for c in text if c not in string.punctuation])\r\n",
    "    return text\r\n",
    "\r\n",
    "# function to remove stopwords\r\n",
    "def remove_stopwords(text):\r\n",
    "    # convert sentence into token of words\r\n",
    "    tokens = tokenizer.tokenize(text)\r\n",
    "    tokens = [token.strip() for token in tokens]\r\n",
    "    # check in lowercase \r\n",
    "    t = [token for token in tokens if token.lower() not in stopword_list]\r\n",
    "    text = ' '.join(t)    \r\n",
    "    return text\r\n",
    "\r\n",
    "# function to remove special characters\r\n",
    "def remove_extra_whitespace_tabs(text):\r\n",
    "    #pattern = r'^\\s+$|\\s+$'\r\n",
    "    pattern = r'^\\s*|\\s\\s*'\r\n",
    "    return re.sub(pattern, ' ', text).strip()\r\n",
    "\r\n",
    "# function to remove special characters\r\n",
    "def to_lowercase(text):\r\n",
    "    return text.lower()\r\n",
    "\r\n",
    "# function to remove numbers\r\n",
    "def remove_numbers(text):\r\n",
    "    # define the pattern to keep\r\n",
    "    pattern = r'[^a-zA-z.,!?/:;\\\"\\'\\s]' \r\n",
    "    return re.sub(pattern, '', text)\r\n",
    "\r\n",
    "nlp = spacy.load(\"en_core_web_sm\")\r\n",
    "\r\n",
    "# function to remove special characters\r\n",
    "def get_lem(text):\r\n",
    "    text = nlp(text)\r\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\r\n",
    "    return text\r\n",
    "    \r\n",
    "spell = Speller(lang='en')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "source": [
    "\r\n",
    "print(imdb[:10])\r\n",
    "X = imdb['sentence'].map(remove_extra_whitespace_tabs)\r\n",
    "X = X.map(remove_stopwords)\r\n",
    "X = X.map(get_lem)\r\n",
    "X = X.map(to_lowercase)\r\n",
    "print(X[:10])\r\n",
    "\r\n",
    "tfidf = TfidfVectorizer()\r\n",
    "X = X\r\n",
    "y = imdb['sentiment']\r\n",
    "X = tfidf.fit_transform(X)\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                            sentence  sentiment\n",
      "0  A very, very, very slow-moving, aimless movie ...          0\n",
      "1  Not sure who was more lost - the flat characte...          0\n",
      "2  Attempting artiness with black & white and cle...          0\n",
      "3       Very little music or anything to speak of.            0\n",
      "4  The best scene in the movie was when Gerardo i...          1\n",
      "5  The rest of the movie lacks art, charm, meanin...          0\n",
      "6                                Wasted two hours.            0\n",
      "7  Saw the movie today and thought it was a good ...          1\n",
      "8                               A bit predictable.            0\n",
      "9  Loved the casting of Jimmy Buffet as the scien...          1\n",
      "0    , , slow - move , aimless movie distress , dri...\n",
      "1    not sure lose - flat character audience , near...\n",
      "2    attempt artiness black & amp ; white clever ca...\n",
      "3                        little music anything speak .\n",
      "4    good scene movie gerardo try find song keep ru...\n",
      "5    rest movie lack art , charm , mean ... ' empti...\n",
      "6                                     waste two hour .\n",
      "7    see movie today think good effort , good messa...\n",
      "8                                    bit predictable .\n",
      "9             love cast jimmy buffet science teacher .\n",
      "Name: sentence, dtype: object\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0)\r\n",
    "\r\n",
    "clf = LinearSVC()\r\n",
    "\r\n",
    "clf.fit(X_train, y_train)\r\n",
    "\r\n",
    "y_pred = clf.predict(X_test)\r\n",
    "print(classification_report(y_test, y_pred))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.67      0.78        84\n",
      "           1       0.69      0.94      0.79        66\n",
      "\n",
      "    accuracy                           0.79       150\n",
      "   macro avg       0.81      0.80      0.79       150\n",
      "weighted avg       0.83      0.79      0.79       150\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "source": [
    "\r\n",
    "print(amazon[:10])\r\n",
    "X = amazon['sentence'].map(remove_extra_whitespace_tabs)\r\n",
    "X = X.map(remove_stopwords)\r\n",
    "X = X.map(get_lem)\r\n",
    "X = X.map(to_lowercase)\r\n",
    "print(X[:10])\r\n",
    "\r\n",
    "tfidf = TfidfVectorizer()\r\n",
    "X = X\r\n",
    "y = amazon['sentiment']\r\n",
    "X = tfidf.fit_transform(X)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                            sentence  sentiment\n",
      "0  A very, very, very slow-moving, aimless movie ...          0\n",
      "1  Not sure who was more lost - the flat characte...          0\n",
      "2  Attempting artiness with black & white and cle...          0\n",
      "3       Very little music or anything to speak of.            0\n",
      "4  The best scene in the movie was when Gerardo i...          1\n",
      "5  The rest of the movie lacks art, charm, meanin...          0\n",
      "6                                Wasted two hours.            0\n",
      "7  Saw the movie today and thought it was a good ...          1\n",
      "8                               A bit predictable.            0\n",
      "9  Loved the casting of Jimmy Buffet as the scien...          1\n",
      "0                    way plug us unless go converter .\n",
      "1                        good case , excellent value .\n",
      "2                                      great jawbone .\n",
      "3    tie charger conversation last 45 minute . majo...\n",
      "4                                          mic great .\n",
      "5       jiggle plug get line right get decent volume .\n",
      "6    several dozen several hundred contact , imagin...\n",
      "7                                razr owner ... must !\n",
      "8                         needless say , waste money .\n",
      "9                                 waste money time ! .\n",
      "Name: sentence, dtype: object\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.91      0.84        96\n",
      "           1       0.90      0.77      0.83       104\n",
      "\n",
      "    accuracy                           0.83       200\n",
      "   macro avg       0.84      0.84      0.83       200\n",
      "weighted avg       0.84      0.83      0.83       200\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0)\r\n",
    "\r\n",
    "clf = LinearSVC()\r\n",
    "\r\n",
    "clf.fit(X_train, y_train)\r\n",
    "\r\n",
    "y_pred = clf.predict(X_test)\r\n",
    "print(classification_report(y_test, y_pred))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.91      0.84        96\n",
      "           1       0.90      0.77      0.83       104\n",
      "\n",
      "    accuracy                           0.83       200\n",
      "   macro avg       0.84      0.84      0.83       200\n",
      "weighted avg       0.84      0.83      0.83       200\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "source": [
    "\r\n",
    "print(yelp[:10])\r\n",
    "X = yelp['sentence'].map(remove_extra_whitespace_tabs)\r\n",
    "X = X.map(remove_punctuation)\r\n",
    "#X = X.map(remove_stopwords)\r\n",
    "X = X.map(get_lem)\r\n",
    "X = X.map(to_lowercase)\r\n",
    "print(X[:10])\r\n",
    "\r\n",
    "tfidf = TfidfVectorizer(max_df=0.8)\r\n",
    "X = X\r\n",
    "y = yelp['sentiment']\r\n",
    "X = tfidf.fit_transform(X)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                            sentence  sentiment\n",
      "0                           Wow... Loved this place.          1\n",
      "1                                 Crust is not good.          0\n",
      "2          Not tasty and the texture was just nasty.          0\n",
      "3  Stopped by during the late May bank holiday of...          1\n",
      "4  The selection on the menu was great and so wer...          1\n",
      "5     Now I am getting angry and I want my damn pho.          0\n",
      "6              Honeslty it didn't taste THAT fresh.)          0\n",
      "7  The potatoes were like rubber and you could te...          0\n",
      "8                          The fries were great too.          1\n",
      "9                                     A great touch.          1\n",
      "0                                  wow love this place\n",
      "1                                    crust be not good\n",
      "2              not tasty and the texture be just nasty\n",
      "3    stop by during the late may bank holiday off r...\n",
      "4    the selection on the menu be great and so be t...\n",
      "5            now i be get angry and i want my damn pho\n",
      "6                   honeslty it do nt taste that fresh\n",
      "7    the potato be like rubber and you could tell t...\n",
      "8                                 the fry be great too\n",
      "9                                        a great touch\n",
      "Name: sentence, dtype: object\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0)\r\n",
    "\r\n",
    "clf = LinearSVC()\r\n",
    "\r\n",
    "clf.fit(X_train, y_train)\r\n",
    "\r\n",
    "y_pred = clf.predict(X_test)\r\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.88      0.85        97\n",
      "           1       0.88      0.83      0.85       103\n",
      "\n",
      "    accuracy                           0.85       200\n",
      "   macro avg       0.85      0.85      0.85       200\n",
      "weighted avg       0.85      0.85      0.85       200\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.1",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit"
  },
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}